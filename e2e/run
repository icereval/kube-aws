#!/bin/bash

KUBE_AWS_CMD=${KUBE_AWS_CMD:-$GOPATH/src/github.com/coreos/kube-aws/bin/kube-aws}
E2E_DIR=$(cd $(dirname $0); pwd)
WORK_DIR=${E2E_DIR}/assets/${KUBE_AWS_CLUSTER_NAME}
TESTINFRA_DIR=${E2E_DIR}/testinfra
KUBE_AWS_NODE_POOL_INDEX=${KUBE_AWS_NODE_POOL_INDEX:-1}
KUBE_AWS_NODE_POOL_NAME=${KUBE_AWS_CLUSTER_NAME}-nodepool${KUBE_AWS_NODE_POOL_INDEX}
NODE_POOL_ASSETS_DIR=${E2E_DIR}/assets/${KUBE_AWS_CLUSTER_NAME}/node-pools/${KUBE_AWS_NODE_POOL_NAME}
KUBE_AWS_TEST_INFRA_STACK_NAME=${KUBE_AWS_TEST_INFRA_STACK_NAME:-${KUBE_AWS_CLUSTER_NAME}-testinfra}
SRC_DIR=$(cd $(dirname $0); cd ..; pwd)
KUBECONFIG=${WORK_DIR}/kubeconfig

export KUBECONFIG

USAGE_EXAMPLE="KUBE_AWS_CLUSTER_NAME=kubeawstest1 KUBE_AWS_KEY_NAME=name/of/ec2/key KUBE_AWS_KMS_KEY_ARN=arn:aws:kms:us-west-1:<account id>:key/your-key KUBE_AWS_REGION=us-west-1 KUBE_AWS_AVAILABILITY_ZONE=us-west-1b ./$0 [prepare|configure|start|all]"

if [ "${KUBE_AWS_CLUSTER_NAME}" == "" ]; then
  echo KUBE_AWS_CLUSTER_NAME is not set. Run this command like $USAGE_EXAMPLE 1>&2
  exit 1
fi

if [ "${KUBE_AWS_KEY_NAME}" == "" ]; then
  echo KUBE_AWS_KEY_NAME is not set. Run this command like $USAGE_EXAMPLE 1>&2
  exit 1
fi

if [ "${KUBE_AWS_REGION}" == "" ]; then
  echo KUBE_AWS_REGION is not set. Run this command like $USAGE_EXAMPLE 1>&2
  exit 1
fi

if [ "${KUBE_AWS_AVAILABILITY_ZONE}" == "" ]; then
  echo KUBE_AWS_REGION is not set. Run this command like $USAGE_EXAMPLE 1>&2
  exit 1
fi

if [ ! -e "${KUBE_AWS_CMD}" ]; then
  echo ${KUBE_AWS_CMD} does not exist. 1>&2
  exit 1
fi

KUBE_AWS_VERSION=$($KUBE_AWS_CMD version)
echo Using the kube-aws command at ${KUBE_AWS_CMD}"($KUBE_AWS_VERSION)". Set KUBE_AWS_CMD=path/to/kube-aws to override.

EXTERNAL_DNS_NAME=${KUBE_AWS_CLUSTER_NAME}.${KUBE_AWS_DOMAIN}
echo The kubernetes API would be accessible via ${EXTERNAL_DNS_NAME}

KUBE_AWS_S3_URI=${KUBE_AWS_S3_DIR_URI}/${KUBE_AWS_CLUSTER_NAME}
echo CloudFormation stack template would be uploaded to ${KUBE_AWS_S3_URI}

build() {
  echo Building kube-aws
  cd ${SRC_DIR}
  ./build
}

prepare() {
  echo Creating or ensuring existence of the kube-aws assets directory ${WORK_DIR}
  mkdir -p ${WORK_DIR}
}

configure() {
  cd ${WORK_DIR}

  ${KUBE_AWS_CMD} init \
    --cluster-name ${KUBE_AWS_CLUSTER_NAME} \
    --external-dns-name ${EXTERNAL_DNS_NAME} \
    --region ${KUBE_AWS_REGION} \
    --availability-zone ${KUBE_AWS_AVAILABILITY_ZONE} \
    --key-name ${KUBE_AWS_KEY_NAME} \
    --kms-key-arn ${KUBE_AWS_KMS_KEY_ARN}

  echo "hostedZoneId: ${KUBE_AWS_HOSTED_ZONE_ID}" >> cluster.yaml
  echo 'createRecordSet: true' >> cluster.yaml

  # required to run kube-aws update
  echo 'workerCount: 2' >> cluster.yaml
  echo 'controllerCount: 2' >> cluster.yaml

  if [ "${KUBE_AWS_USE_CALICO}" != "" ]; then
    echo 'useCalico: true' >> cluster.yaml
  fi

  customize_worker

  ${KUBE_AWS_CMD} render

  ${KUBE_AWS_CMD} validate --s3-uri ${KUBE_AWS_S3_URI}

  echo Generated configuration files in ${WORK_DIR}:
  find .
}

customize_worker() {
  echo Writing to $(pwd)/cluster.yaml

  if [ "${KUBE_AWS_DEPLOY_TO_EXISTING_VPC}" != "" ]; then
    echo "vpcId: $(testinfra_vpc)" >> cluster.yaml
    echo "routeTableId: $(testinfra_public_routetable)" >> cluster.yaml
    echo -e "workerSecurityGroupIds:\n- $(testinfra_glue_sg)" >> cluster.yaml
  fi

  #
  # Optionally enable experimental other features
  #

  echo -e 'experimental:\n  nodeDrainer:\n    enabled: true' >> cluster.yaml
  if [ "${KUBE_AWS_WAIT_SIGNAL_ENABLED}" != "" ]; then
    echo -e '  waitSignal:\n    enabled: true' >> cluster.yaml
  fi
  if [ "${KUBE_AWS_AWS_NODE_LABELS_ENABLED}" != "" ]; then
    echo -e '  awsNodeLabels:\n    enabled: true' >> cluster.yaml
  fi
  if [ "${KUBE_AWS_NODE_LABELS_ENABLED}" != "" ]; then
    echo -e '  nodeLabels:\n    kube-aws.coreos.com/role: worker' >> cluster.yaml
  fi
  if [ "${KUBE_AWS_AWS_ENV_ENABLED}" != "" ]; then
    echo -e "  awsEnvironment:\n    enabled: true\n    environment:\n      CFNSTACK: '{\"Ref\":\"AWS::StackId\"}'" >> cluster.yaml
  fi
  if [ "${KUBE_AWS_DEPLOY_TO_EXISTING_VPC}" != "" ]; then
    echo -e "  loadBalancer:\n    enabled: true\n    names:\n      - $(testinfra_public_lb)\n    securityGroupIds:\n      - $(testinfra_public_lb_backend_sg)" >> cluster.yaml
  fi


  #
  # Optionally enable experimental worker features
  #

  echo -e "worker:" >> cluster.yaml
}

clean() {
  cd ${WORK_DIR}/..
  if [ -d "${KUBE_AWS_CLUSTER_NAME}" ]; then
    echo Removing the directory "${WORK_DIR}"
    rm -Rf ./${KUBE_AWS_CLUSTER_NAME}/*
  fi
}

up() {
  cd ${WORK_DIR}

  starttime=$(date +%s)

  ${KUBE_AWS_CMD} up --s3-uri ${KUBE_AWS_S3_URI}

  set +vx

  printf 'Waiting for the Kubernetes API to be accessible'

  while ! kubectl get no 2>/dev/null; do
    sleep 10
    printf '.'
  done

  endtime=$(date +%s)

  echo Done. it took $(($endtime - $starttime)) seconds until kubeapiserver to be in service.

  set -vx
}

destroy() {
  aws cloudformation delete-stack --stack-name ${KUBE_AWS_CLUSTER_NAME}
  aws cloudformation wait stack-delete-complete --stack-name ${KUBE_AWS_CLUSTER_NAME}
}

update() {
  cd ${WORK_DIR}

  ${KUBE_AWS_CMD} update --s3-uri ${KUBE_AWS_S3_URI} || true

  SED_CMD="sed -e 's/workerCount: 2/workerCount: 3/' -e 's/controllerCount: 2/controllerCount: 3/'"
  diff --unified cluster.yaml <(cat cluster.yaml | sh -c "${SED_CMD}") || true
  sh -c "${SED_CMD} -i bak cluster.yaml"
  ${KUBE_AWS_CMD} update --s3-uri ${KUBE_AWS_S3_URI}
  aws cloudformation wait stack-update-complete --stack-name ${KUBE_AWS_CLUSTER_NAME}

  printf 'Waiting for the Kubernetes API to be accessible'
  while ! kubectl get no 2>/dev/null; do
    sleep 10
    printf '.'
  done
  echo done
}

test-destruction() {
  aws cloudformation wait stack-delete-complete --stack-name ${KUBE_AWS_CLUSTER_NAME}
}

# Usage: DOCKER_REPO=quay.io/mumoshu/ SSH_PRIVATE_KEY=path/to/private/key ./e2e run conformance
conformance() {
  cd ${E2E_DIR}/kubernetes

  if [ "$DOCKER_REPO" == "" ]; then
    echo DOCKER_REPO is not set.
    exit 1
  fi

  if [ "$SSH_PRIVATE_KEY" == "" ]; then
    echo SSH_PRIVATE_KEY is not set.
    exit 1
  fi

  if [ ! -f "$SSH_PRIVATE_KEY" ]; then
    echo ${SSH_PRIVATE_KEY} does not exist.
    exit 1
  fi

  echo Opening ingress on 4194 and 10250...

  # Authorize these ingresses for E2E testing or it'll end up failing like:
  #
  # Summarizing 2 Failures:
  #
  # [Fail] [k8s.io] Proxy version v1 [It] should proxy logs on node using proxy subresource [Conformance]
  # /go/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/proxy.go:325
  #
  # [Fail] [k8s.io] Proxy version v1 [It] should proxy logs on node with explicit kubelet port [Conformance]
  # /go/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/proxy.go:325
  #
  # Ran 117 of 473 Specs in 3529.785 seconds
  # FAIL! -- 115 Passed | 2 Failed | 0 Pending | 356 Skipped --- FAIL: TestE2E (3529.90s)
  #
  group_id=$(aws cloudformation --output json describe-stack-resources --stack-name ${KUBE_AWS_CLUSTER_NAME} | jq -r '.StackResources[] | select(.LogicalResourceId == "SecurityGroupController").PhysicalResourceId')
  aws ec2 authorize-security-group-ingress --group-id ${group_id} --protocol tcp --port 4194 --source-group ${group_id} || echo 'skipping authorization for 4194'
  aws ec2 authorize-security-group-ingress --group-id ${group_id} --protocol tcp --port 10250 --source-group ${group_id} || echo 'skipping authorization for 10250'

  master_host=$(controller_host)

  echo Connecting to $master_host via SSH

  KUBE_AWS_ASSETS=${WORK_DIR} MASTER_HOST=$master_host make run-remotely
}

conformance_result() {
  cd ${E2E_DIR}/kubernetes

  master_host=$(controller_host)

  echo Connecting to $master_host via SSH

  KUBE_AWS_ASSETS=${WORK_DIR} MASTER_HOST=$master_host make show-log
}

first_host_for_asg() {
  aws ec2 describe-instances --output json --query "Reservations[].Instances[]
      | [?Tags[?Key==\`aws:cloudformation:stack-name\`].Value|[0]==\`${KUBE_AWS_CLUSTER_NAME}\`]
      | [?Tags[?Key==\`aws:cloudformation:logical-id\`].Value|[0]==\`${1}\`][]
      | [?State.Name==\`running\`][]
      | []" | jq -r 'map({InstanceId: .InstanceId, PublicIpAddress: .PublicIpAddress}) | first | .PublicIpAddress'
}

controller_host() {
  first_host_for_asg AutoScaleController
}

worker_host() {
  first_host_for_asg AutoScaleWorker
}

ssh_controller() {
  ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -i ${KUBE_AWS_SSH_KEY} core@$(controller_host) "$@"
}

ssh_worker() {
  ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -i ${KUBE_AWS_SSH_KEY} core@$(worker_host) "$@"
}

nodepool_init() {
  cd ${WORK_DIR}

  ${KUBE_AWS_CMD} node-pools init --node-pool-name ${KUBE_AWS_NODE_POOL_NAME} \
      --availability-zone ${KUBE_AWS_AVAILABILITY_ZONE} \
      --key-name ${KUBE_AWS_KEY_NAME} \
      --kms-key-arn ${KUBE_AWS_KMS_KEY_ARN}

  cd ${NODE_POOL_ASSETS_DIR}

  echo -e "instanceCIDR: 10.0.${KUBE_AWS_NODE_POOL_INDEX}.0/24" >> ${NODE_POOL_ASSETS_DIR}/cluster.yaml

  customize_worker

  if [ "${KUBE_AWS_SPOT_FLEET_ENABLED}" != "" ]; then
    echo -e "  spotFleet:\n    targetCapacity: 3\n" >> cluster.yaml
  fi
  if [ "${KUBE_AWS_CLUSTER_AUTOSCALER_ENABLED}" != "" ]; then
    echo -e "  clusterAutoscaler:\n    minSize: 1\n    maxSize: 2" >> cluster.yaml
  fi
}

nodepool_render() {
  cd ${WORK_DIR}

  ${KUBE_AWS_CMD} node-pools render stack --node-pool-name ${KUBE_AWS_NODE_POOL_NAME}
}

nodepool_validate() {
  cd ${WORK_DIR}

  ${KUBE_AWS_CMD} node-pools validate --node-pool-name ${KUBE_AWS_NODE_POOL_NAME} --s3-uri ${KUBE_AWS_S3_URI}
}

nodepool_up() {
  cd ${WORK_DIR}

  ${KUBE_AWS_CMD} node-pools up --node-pool-name ${KUBE_AWS_NODE_POOL_NAME} --export
  ${KUBE_AWS_CMD} node-pools up --node-pool-name ${KUBE_AWS_NODE_POOL_NAME} --s3-uri ${KUBE_AWS_S3_URI}
}

nodepool_update() {
  cd ${WORK_DIR}

  pushd ${NODE_POOL_ASSETS_DIR}

  if [ "${KUBE_AWS_SPOT_FLEET_ENABLED}" ]; then
    SED_CMD="sed -e 's/targetCapacity: 3/targetCapacity: 5/'"
    diff --unified cluster.yaml <(cat cluster.yaml | sh -c "${SED_CMD}") || true
    sh -c "${SED_CMD} -i bak cluster.yaml"
  else
    echo 'workerCount: 2' >> cluster.yaml
  fi

  popd

  ${KUBE_AWS_CMD} node-pools update --node-pool-name ${KUBE_AWS_NODE_POOL_NAME} --s3-uri ${KUBE_AWS_S3_URI}
}

nodepool_destroy() {
  cd ${WORK_DIR}

  status=$(aws cloudformation describe-stacks --stack-name ${KUBE_AWS_NODE_POOL_NAME} --output json | jq -rc '.Stacks[0].StackStatus')

  if [ "$status" != "DELETE_IN_PROGRESS" ]; then
    ${KUBE_AWS_CMD} node-pools destroy --node-pool-name ${KUBE_AWS_NODE_POOL_NAME}
  else
    aws cloudformation wait stack-delete-complete \
        --stack-name ${KUBE_AWS_NODE_POOL_NAME}
  fi
}

nodepool() {
  cd ${WORK_DIR}

  nodepool_init
  nodepool_render
  nodepool_validate
  nodepool_up
  nodepool_update
}

testinfra_up() {
  cd ${TESTINFRA_DIR}

  aws cloudformation create-stack \
    --template-body file://$(pwd)/stack-template.yaml \
    --stack-name ${KUBE_AWS_TEST_INFRA_STACK_NAME} \
    --parameter ParameterKey=AZ1,ParameterValue=${KUBE_AWS_AZ_1}
  aws cloudformation wait stack-create-complete \
    --stack-name ${KUBE_AWS_TEST_INFRA_STACK_NAME}
}

testinfra_update() {
  cd ${TESTINFRA_DIR}

  aws cloudformation update-stack \
    --template-body file://$(pwd)/stack-template.yaml \
    --stack-name ${KUBE_AWS_TEST_INFRA_STACK_NAME} \
    --parameter ParameterKey=AZ1,ParameterValue=${KUBE_AWS_AZ_1}
  aws cloudformation wait stack-update-complete \
    --stack-name ${KUBE_AWS_TEST_INFRA_STACK_NAME}
}

testinfra_destroy() {
  aws cloudformation delete-stack --stack-name ${KUBE_AWS_TEST_INFRA_STACK_NAME}
  aws cloudformation wait stack-delete-complete \
    --stack-name ${KUBE_AWS_TEST_INFRA_STACK_NAME}
}

testinfra_output() {
  aws cloudformation describe-stacks --stack-name ${KUBE_AWS_TEST_INFRA_STACK_NAME} | jq -r '.Stacks[0].Outputs[] | select(.OutputKey == "'$1'").OutputValue'
}

testinfra_vpc() {
  testinfra_output VPC
}

testinfra_public_routetable() {
  testinfra_output PublicRouteTable
}

testinfra_public_lb_backend_sg() {
  testinfra_output PublicLBBackendSG
}

testinfra_public_lb() {
  testinfra_output PublicLB
}

testinfra_glue_sg() {
  testinfra_output GlueSG
}

all() {
  build

  if [ "${KUBE_AWS_DEPLOY_TO_EXISTING_VPC}" != "" ]; then
    testinfra_up
  fi

  prepare
  configure
  up
  nodepool
  update
  conformance
}

all_destroy() {
  nodepool_destroy
  destroy
  if [ "${KUBE_AWS_DEPLOY_TO_EXISTING_VPC}" != "" ]; then
    testinfra_destroy
  fi
}

if [ "$1" == "" ]; then
  echo Usage: $USAGE_EXAMPLE
  exit 1
fi

set -vxe

"$@"
